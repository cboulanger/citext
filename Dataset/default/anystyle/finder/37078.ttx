text          | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
              | Asking Survey Respondents about Reasons for Their
              | Behavior: A Split Ballot Experiment in Ethiopia
              | Charles Q. Lau, Survey Research Division, RTI International
              | Gretchen McHenry, Survey Research Division, RTI International
              | 15.01.2014
              | How to cite this article: Lau C.Q., & McHenry G. (2014). Asking Survey Respondents about
              | Reasons for Their Behavior: A Split Ballot Experiment in Ethiopia, Survey Methods: Insights
              | from the Field. Retrieved from http://surveyinsights.org/?p=2914
              | Abstract
              | When policymakers design programs and policies, they often want to understand why
              | individuals engage in particular behaviors. Collecting survey data about respondentsʼ
              | reasons for their behavior presents important challenges, and there is little methodological
              | research on this topic. We conducted an experiment to investigate the best practices for
              | asking questions about respondentsʼ reasons for their behavior. We embedded a split ballot
              | experiment in a face-to-face survey of 608 entrepreneurs in Ethiopia. Respondents were
              | asked questions about why they did not engage in three business practices (advertising,
              | sharing product storage, and switching suppliers). When asked these questions, respondents
              | were randomly assigned to one of three conditions: close-ended questions, open-ended
              | questions with interviewer probing, and open-ended questions without probing. Respondents
              | endorsed more responses when asked close-ended (versus open-ended) questions.
              | Close-ended responses produced higher rates of socially undesirable responses and fewer
              | “other” responses. Notably, probing had no effect on the number or types of responses given.
              | Our results suggest some best practices for asking respondents questions about reasons for
              | their behavior.
              | Keywords
              | Acknowledgement
              | close-ended, developing country, entrepreneurs, Motivation, open-ended, probing, reason,
              | sensitive, social desirability
              | The authors gratefully acknowledge support from RTI International for funding the Kal Addis
              | Business Survey (KABS). We thank Jason Wares for assistance in designing KABS, and
              | Efera Busa and Benyam Lemma for assistance in collecting the KABS data. We also thank
              | Hyunjoo Park of RTI International and two anonymous reviewers for valuable comments on
              | the paper. Any errors in this manuscript are our own.
              | Copyright
              | © the authors 2013. This work is licensed under a Creative Commons Attribution-
              | NonCommercial-NoDerivs 3.0 Unported License (CC BY-NC-ND 3.0)
              | 1 sur 14
              | 15.01.14 09:36
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
              | Introduction
              | When policymakers design programs and policies, they often want to understand why
              | individuals act in particular ways. Although some researchers caution against asking
              | respondents to cite reasons why they do (or do not) engage in behaviors (Pasek and
              | Krosnick 2010: 41; Wilson 2010), data about respondent motivations for their behavior are
              | analytically useful. By understanding the causes of peopleʼs behavior, policymakers can take
              | steps to reduce undesirable behaviors or encourage desirable behaviors. For example,
              | questions in the National Health Interview Survey ask respondents why they delayed seeking
              | medical care, allowing researchers to understand barriers to healthcare access (Centers for
              | Disease Control and Prevention, 2012). The Current Population Survey also asks individuals
              | why they did not vote or register to vote, shedding light on mechanisms underlying political
              | participation (United States Census Bureau, 2010).
              | Given the value of data about individualsʼ motivations for behavior, it is notable that there is
              | little research on best practices for designing these questions. To address this gap, we
              | embedded a split ballot experiment in a face-to-face survey of 608 entrepreneurs in Ethiopia.
              | Conducting the survey in a developing country allowed us to study this topic in a context that
              | poses additional challenges to asking such questions. In our survey, we randomly assigned
              | one of three methods for asking respondents about reasons for their behavior. The methods
              | differ in whether questions are close-ended versus open-ended, and whether interviewers
              | probed respondents. Our analysis evaluates the three methods by comparing numbers of
              | endorsed responses and the number of socially desirable responses in particular.
              | Background
              | There are many ways to collect survey data about reasons for respondentʼs behavior. The
              | method we adopt in this paper involves pre-specifying a list of possible reasons for behavior
              | on the instrument, and then having interviewers record whether each response applies or not
              | (using yes/no responses for each item).[1] When designing this type of question, researchers
              | must make two key decisions (Wilson 2010). First, should interviewers ask close-ended
              | questions—reading each possible response and then recording a yes/no response for each?
              | Or should interviewers ask open-ended questions and then record yes/no responses based
              | on the respondentʼs open-ended answer? Second, if open-ended questions are used, should
              | interviewers probe respondents for clariﬁcation? In the following sections, we draw from
              | previous literature to develop expectations about the advantages and disadvantages of
              | different types of close-ended and open-ended questions.
              | Close-Ended Versus Open-Ended Questions
              | Asking close-ended questions (rather than open-ended ones) is a form of standardized
              | interviewing, in which each respondent hears the exact same question and response options,
              | regardless of the interview ﬂow or tone (Converse and Schuman 1974; Schober and Conrad
              | 2002). This approach has the advantage of encouraging respondents to consider reasons
              | they had not previously thought about. It also encourages respondents to think about the
              | issue from a variety of perspectives, which may result in a greater number of endorsed
              | responses and may also limit “donʼt know” responses. Further, close-ended questions may
              | reduce respondentsʼ concerns about reporting socially undesirable answers. Reading
              | response options may give tacit approval for socially undesirable answers and may help
              | develop a sense of trust between a respondent and the interviewer. It also means that
              | respondents do not have to verbally state a socially undesirable admission about
              | themselves, which is the case in an open-ended question format.
              | 2 sur 14
              | 15.01.14 09:36
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
              | Close-ended questions also have disadvantages, many of which are rectiﬁed by open-ended
              | formats. Reading close-ended response options during the interview can be time consuming
              | and feel repetitive to the respondent. Open-ended questions, in contrast, may be more
              | engaging for respondents because they comport more with conversational norms and allow
              | respondents to better communicate the reasoning behind their behavior (Fowler 1995).
              | Open-ended questions have also been shown to solicit meaningful, salient information from
              | respondents (Geer 1988; Geer 1991). In addition, close-ended questions may suffer from
              | primacy or recency effects, where the ﬁrst (or last) response options are more likely to be
              | endorsed, whereas primacy and recency effects are eliminated with open-ended questions.
              | Finally, reading response options may implicitly convey the researcherʼs values or
              | preferences, potentially biasing respondents in a particular direction. Open-ended questions,
              | in contrast, do not have this limitation, and also provide an opportunity to collect data about
              | issues researchers had not previously considered.
              | Probing
              | If open-ended questions are used, interviewers could simply select the pre-speciﬁed reasons
              | that apply to the respondentsʼ open-ended answer (without probing), or probe for a more
              | complete or detailed response. Probing can facilitate respondent comprehension of the
              | question and may reduce errors in the interviewerʼs coding of responses. An exchange with
              | the interviewer also may encourage respondents to think more deeply about their answers.
              | This increased engagement with the question-answer process, as well as with the
              | interviewer, may yield more endorsed answers, reduce respondent satisﬁcing, and increase
              | reports of socially undesirable behaviors. Schaeffer and Maynard (2008) show that directive
              | probes or requests for conﬁrmation from interviewers increase a respondentʼs likelihood of
              | reporting embarrassing or incriminating responses.
              | There are three potential drawbacks of probing. First, probing gives interviewers more
              | discretion and may lead to increased interviewer errors and variance. For example, Fowler
              | and Mangione (1990) ﬁnd that the number of probes, directive probes used, and occasions
              | where an interviewer failed to probe are associated with increased error. They also suggest
              | that probing may introduce interviewer-level variance, which decreases the efﬁciency of
              | survey estimates. However, Schober and Conradʼs (1997) small-scale experimental study
              | ﬁnds no evidence that probing increases interviewer error or variance. Second, probing may
              | increase the number of “other” responses if the interviewer cannot code the response into
              | one of the pre-existing categories due to the nuanced response from the respondents. Third,
              | the conversational nature of the interview may increase administration time, increasing
              | survey costs and ﬁeld data collection time.
              | In sum, the literature suggests that there are advantages and disadvantages of using
              | close-ended versus open-ended questions, as well as probing versus not probing. Given the
              | lack of research in this area, we designed a split ballot experiment to investigate the quality
              | of data produced by three methods.
              | Experimental Design
              | Data
              | We analyze data from the Kal Addis Business Survey (KABS), a paper-and-pencil interview
              | of 608 entrepreneurs in the Ethiopian capital of Addis Ababa. Eligible respondents were
              | owners or senior managers of small and medium businesses (between 3 and 99 employees)
              | based in Addis Ababa. Examples of businesses in the sample include a restaurant, car repair
              | shop, and a textile manufacturer. The purpose of KABS was to improve sampling and
              | questionnaire design methodologies in developing countries, particularly for surveys of
              | entrepreneurs. The survey measured entrepreneursʼ attitudes and business practices, and
              | 3 sur 14
              | 15.01.14 09:36
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
              | included questions about purchasing raw materials from suppliers, advertising, product
              | storage, among other topics. Professional Ethiopian interviewers with at least three years of
              | interviewing experience administered the survey in the Amharic language in the summer of
              | 2012. All interviewers also participated in a three day training and pre-test of the instrument.
              | Throughout data collection, survey managers held quality review meetings with interviewers
              | to enhance standardization and to answer questions about ﬁeld implementation. The mean
              | administration time was 29 minutes (standard deviation = 9 minutes).
              | Because a sampling frame of entrepreneurs was not available in Addis Ababa, KABS used
              | respondent-driven sampling (RDS). RDS is a method of chain referral sampling that
              | combines a snowball sample with a mathematical model that adjusts for the non-random
              | selection of the initial set of respondents (Heckathorn 1997). To implement RDS in KABS, we
              | initially recruited a convenience sample of 24 individuals through personal networks. These
              | individuals were interviewed and then provided with three invitations to recruit up to three
              | individuals to participate in the study. Each additional wave of recruits was asked to recruit
              | up to three additional individuals. Recruited individuals contacted the ﬁeld data collection
              | teams, who then scheduled and conducted the interview in a location of the respondentʼs
              | choosing. We provided a leather wallet to respondents for completing the survey and mobile
              | phone airtime for referring others to the study. Because our focus is on the internal validity of
              | the split ballot experiment, we do not apply weights from the RDS in the paper.
              | Characteristics of the sample are presented in Table 1.
              | Three-quarters of respondents are male with an average age of 31 years old, reﬂecting the
              | young age of the Ethiopian population. The majority of respondents are owners of the
              | business (82%) versus managers (18%). The sample is comprised of businesses in the
              | manufacturing (14%), service (48%), and trade (39%) sectors. The vast majority of
              | businesses were proﬁtable in the past year, and on average, businesses had eight
              | employees and were six years old.
              | Table 1. Sample characteristics
              | Respondent Characteristics
              | Gender (n = 608)
              | Male
              | Female
              | Total %
              | Educational attainment (n =
              | 608)
              | Did not complete secondary
              | Secondary school
              | Vocational or some university
              | Graduate degree or higher
              | Total %
              | 76%
              | 24%
              | 100%
              | 15%
              | 34%
              | 31%
              | 20%
              | 100%
              | 14%
              | 48%
              | 39%
              | 100%
              | 32%
              | 21%
              | 13%
              | 17%
              | 16%
              | 4 sur 14
              | 15.01.14 09:36
              | Business Characteristics
              | Sector (n = 608)
              | Manufacturing
              | Service
              | Trade
              | Total %
              | Annual revenue in dollars (n = 539)
              | Less than $2778
              | $2778 – $5,555
              | $5,556 – $13,889
              | $13,890 – $41,667
              | Over $41,667
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
              | Position in business (n = 608)
              | Total %
              | Owner
              | Total %
              | Senior day-to-day manager
              | Age in years (n = 608)
              | Standard deviation
              | Hours worked/week (n = 596)
              | Standard deviation
              | 82%
              | 18%
              | 100%
              | 31.2
              | 6.9
              | 55.0
              | 20.0
              | Proﬁt last year (n = 583)
              | Made money
              | Lost money
              | Broke even
              | Total %
              | Number of employees
              | (n = 608)
              | Standard deviation
              | Mean business age
              | (years) (n = 606)
              | Standard deviation
              | 100%
              | 70%
              | 7%
              | 23%
              | 100%
              | 7.9
              | 12.3
meta          | 6
              | 6
text          | Note: The total sample size for KABS sample is 608. The valid sample size for each
              | variable is indicated in table. Percentages may not sum to 100 due to rounding.
              | KABS included questions about three business practices: advertising, switching to a new
              | supplier to buy raw materials, and sharing product storage with another business. We
              | present the exact question wording for these questions in the Appendix. These three
              | practices facilitate economic growth and are practices that policymakers would like to
              | encourage in developing countries. Therefore, understanding why individuals do not engage
              | in these practices is important for policymakers who design interventions to stimulate
              | economic growth. In different parts of the interview, respondents were asked if they engaged
              | in these business practices. Those who said they did not take part in each business practice
              | were asked why not. We generated pre-speciﬁed reasons for each behavior during formative
              | research, which involved in-depth interviews with entrepreneurs, as well as a review of
              | literature on entrepreneurship in Ethiopia. We modiﬁed these reasons throughout the
              | pre-testing process.
              | Split Ballot Design
              | We developed three separate instruments, each with a different method of asking questions
              | about reasons for respondentʼs behavior. Respondents were randomly assigned to one of
              | three methods (Table 2). Each respondent was assigned to the same method across all three
              | business practices based on their respondent ID number (itself randomly assigned). The
              | randomization was successful in that there were no signiﬁcant correlations between
              | questionnaire version and respondent or business characteristics. Full tables are available
              | from the authors upon request.
              | Table 2 shows that in the close-ended method, interviewers read every pre-speciﬁed
              | response option while asking respondents a series of yes/no questions about whether the
              | option applied or not. This method is the norm in social surveys and reﬂects standardized
              | interviewing practices (Groves et al. 2009). We read the potential reasons orally (rather than
              | using a showcard) because of the survey populationʼs lower levels of literacy and
              | unfamiliarity with showcards.
              | 5 sur 14
              | 15.01.14 09:36
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
              | Table 2. Three methods of asking about reasons for behavior
              | Close-ended
              | Open-ended with Open-ended
              | probing without probing
              | Interviewer reads
              | response options
              | Interviewer probes
              | Number of respondents
              | Yes
              | If needed
meta          | 203
text          | No
              | Yes
meta          | 203
text          | No
              | No
meta          | 202
text          | In the open-ended with probing method, interviewers asked an open-ended question instead
              | of reading the response options. The interviewer then coded the respondentʼs open-ended
              | answer into the pre-speciﬁed options, and probed the respondent as needed. The interviewer
              | did not record the verbatim open-ended response. Interviewers were trained to adopt
              | conversational interviewing practices when probing (Schober 1998), and used non-directive,
              | neutral probes to clarify unclear or inadequate responses. Examples of probes included
              | repeating the question, asking a general question, or asking a respondent to clarify a
              | response. In the open-ended without probing method, interviewers asked an open-ended
              | question, coded the open-ended data into the pre-speciﬁed response options, and did not
              | probe. Again, the interviewer did not collect the verbatim response. This method combines
              | elements of standardization (i.e., no interviewer-respondent discussion) and conversational
              | interviewing (i.e., interviewer has discretion to select the appropriate response.) All three
              | methods contained an “other (specify)” response. During preliminary analysis, we recoded
              | some “other” responses into existing pre-speciﬁed categories or created new categories
              | when the other (specify) meaning was unambiguous.
              | Hypotheses
              | Our analysis seeks to identify the method that produces the most useful data about why
              | individuals do not engage in the three business practices. Because obtaining validation data
              | for this type of information is difﬁcult, we focus on the number of endorsed responses and
              | socially undesirable responses in particular. Another possible indicator is timing data, but
              | because KABS used paper-and-pencil interviewing (like most surveys in developing
              | countries), timing data on individual questions were not available. Below, we describe each
              | indicator and present hypotheses.
              | Number of endorsed responses: The number of responses that respondents select is
              | indicative of greater engagement with the subject matter. A greater number of responses is
              | also analytically useful because it helps analysts understand multiple inﬂuences on behavior.
              | Hypothesis 1: The close-ended method will result in greater number of endorsed reasons
              | than either open-ended method because respondents must consider each option separately.
              | Support for this reasoning comes from the web survey literature, which shows that
              | respondents endorse more responses when presented with a yes/no matrix (that requires an
              | answer for each response) rather than a “check all” list (Smyth et al. 2006).
              | 6 sur 14
              | 15.01.14 09:36
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
              | Hypothesis 2: Open-ended with probing will lead to a greater number of endorsements than
              | open-ended without probing. During probing, interviewers may encourage respondents to
              | think about the issue from multiple angles and therefore provide more responses.
              | Socially undesirable reporting: We assume that respondents are reluctant to endorse
              | responses that are socially undesirable, and that increases in socially undesirable reporting
              | reﬂect a more preferable method. This logic has been widely used in other areas, such as
              | mode effects on reports of sexual activity (Tourangeau and Smith 1996) and smoking
              | (Currivan et al. 2004). We include a range of socially undesirable measures in our study,
              | ranging from more sensitive (e.g., reporting distrust of others) to less sensitive (e.g.,
              | reporting lack of knowledge about an issue).
              | Hypothesis 3: The close-ended method will yield more socially undesirable reporting than
              | either open-ended method because the interviewer-supplied responses give tacit approval to
              | the possibility of the response. In addition, the respondent only has to say “yes” to endorse a
              | socially undesirable behavior in the close-ended method, whereas the respondent must
              | verbalize the socially undesirable behavior in the open-ended methods.
              | Hypothesis 4: Open-ended with probing will lead to more socially undesirable reports than
              | open-ended without probing. Probing may help an interviewer build rapport with a
              | respondent and uncover issues that respondents do not immediately discuss.
              | Results
              | Number of Endorsed Reasons
              | In Table 3, we present the number of reasons endorsed by each experimental group,
              | separately for the three business practices. We report the percentage of respondents that
              | endorsed more than one reason, the percentage distribution of the number of reasons
              | endorsed, and the mean number of reasons. For “more than one reason” and “mean number
              | of reasons,” we use superscripts to highlight statistically signiﬁcant differences (p < .05) that
              | were obtained through post-hoct-tests.
              | Table 3: Number of Reasons Endorsed (Percentages unless noted)
              | A. Reasons for Not
              | Advertising
              | More than one reason
              | Number of reasons
meta          | 0
              | 1
              | 2
              | 3
              | 4
text          | Close-
              | ended(n =
              | 167)
              | 34b, c
meta          | 1
              | 66
              | 25
              | 7
              | 1
text          | Open-ended
              | with
              | probing(n =
              | 165)
              | Open-ended
              | without
              | probing(n =
              | 163)
              | 18a
meta          | 2
              | 80
              | 15
              | 2
              | 1
text          | 23a
meta          | 4
              | 73
              | 21
              | 2
              | 0
text          | Mean number of
              | reasons (std. dev)
              | 1.4 b, c(.69)
              | 1.2 a(.56)
              | 1.2 a(.53)
              | 7 sur 14
              | 15.01.14 09:36
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
meta          | 7
              | 3
              | 90
              | 7
              | 0
              | 0
              | 5
              | 0
              | 95
              | 5
text          | < 1
              | < 1
meta          | 6
              | 3
              | 90
              | 3
              | 3
              | 0
              | 9
              | 0
              | 91
              | 3
              | 3
              | 3
text          | Close-
              | ended(n =
              | 58)
              | Open-ended
              | with
              | probing(n =
              | 67)
              | Open-ended
              | without
              | probing(n =
              | 62)
              | 1.3 b(.74)
              | 1.0 a(.32)
              | 1.1(.44)
              | Close-
              | ended(n =
              | 37)
              | Open-ended
              | with
              | probing(n =
              | 41)
              | Open-ended
              | without
              | probing(n =
              | 35)
              | B. Reasons for Not
              | Switching Supplier
              | More than one reason
              | Number of reasons
              | Mean number of
              | reasons (std. dev)
              | C. Reasons for Not
              | Sharing Storage
              | More than one reason
              | Number of reasons
meta          | 0
              | 1
              | 2
              | 3
              | 4
              | 0
              | 1
              | 2
              | 3
              | 4
              | 17
              | 3
              | 79
              | 7
              | 9
              | 2
              | 16
              | 3
              | 81
              | 11
              | 5
              | 0
text          | Mean number of
              | reasons (std. dev)
              | 1.2(.57)
              | 1.0(.22)
              | 1.2(.62)
              | a Statistically signiﬁcant difference from close-ended (p < .05)b
              | Statistically signiﬁcant difference from open-ended with probing (p < .05)c
              | Statistically signiﬁcant difference from open-ended without probing (p <
              | .05)
              | In the reasons for not advertising panel, the results show that the close-ended design yielded
              | more reasons than both open-ended methods. In the close-ended group, 34% of
              | respondents provided more than one response, compared to 18% and 23% for the
              | open-ended groups with and without probing, respectively. The differences between the
              | close-ended group and both open-ended groups were statistically signiﬁcant (p < .05). The
              | full distribution shows that the close-ended group reported two reasons 25% of the time,
              | 8 sur 14
              | 15.01.14 09:36
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
              | compared to 15% for the open-ended with probing and 21% for the open-ended without
              | probing group. The close-ended group also provided a higher mean number of reasons than
              | both open-ended groups (p < .05). There was no statistically signiﬁcant difference, however,
              | between the two open-ended groups in the number of endorsed reasons.
              | We observed a similar pattern in the “switching supplier” panel. The close-ended group
              | reported more than one reason in 17% of cases, higher than the open-ended groups with
              | probing (7%) and without probing (6%), though these differences were only marginally
              | statistically signiﬁcant (p < .10). However, the close-ended group had a signiﬁcantly higher (p
              | < .05) mean number of reasons endorsed (1.3) compared to the open-ended with probing
              | group (1.0). The results in the “sharing storage” panel follow the same pattern. The
              | differences, however, are not statistically signiﬁcant, likely due to the small sample sizes.
              | In sum, the close-ended method produced endorsements of more options compared to
              | open-ended methods, supporting Hypothesis 1. The results, however, do not provide support
              | for Hypothesis 2: probing had no effect on the number of reasons respondents endorse.
              | Type of Responses Provided
              | Next, we investigated how question design affected the number of socially undesirable
              | responses provided, separately by the three business practices.
              | Reasons for Not Advertising
              | In Table 4, we show the reasons respondents provided for not advertising, separately by
              | experimental group. Several of these reasons are socially undesirable, such as the reason
              | that advertising might lead to an “increase government inspections or auditing.” This reason
              | is socially taboo because it indirectly refers to bribes: In developing countries such as
              | Ethiopia, advertising increases a businessʼ prominence, making it an easier target for
              | government ofﬁcials to demand bribes through unnecessary inspections or audits.
              | Respondents may not endorse this reason because they prefer to avoid discussing about the
              | sensitive topic of bribes, and also to minimize being perceived as having paid bribes. Of
              | respondents in the close-ended group, 14% cited this reason, twice as high as the
              | open-ended with probing group (7%); this difference was statistically signiﬁcant. Nine percent
              | of the open-ended without probing group mentioned this reason.
              | Table 4. Reasons for Not Advertising, by Experimental Group (Percentages)
              | Open-ended
              | Open-ended without
              | Close-ended with probing probing
              | (n = 167)
              | (n = 165)
              | (n = 163)
              | Too expensive
              | Wouldnʼt help business
              | Would increase
              | government
              | inspections or auditing
meta          | 51
              | 44
text          | 14 b
meta          | 50
              | 35
text          | 7 a
meta          | 50
              | 44
              | 9
text          | Overall χ2
              | χ2(2) = 0.0; p =
              | .97
              | χ2(2) = 4.0; p =
              | .13
              | χ2(2) = 5.1; p =
              | .08
              | 9 sur 14
              | 15.01.14 09:36
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
              | Business is too new or
              | small
              | Never thought about it
              | Other
              | Too complicated or
              | takes too much time
meta          | 13
text          | 8 b
meta          | 7
text          | 5 b
meta          | 13
text          | 3 a
              | 12c
              | 1 a
meta          | 8
              | 5
              | 2
text          | 5 b
              | χ2(2) = 3.0; p =
              | .22
              | χ2(2) = 4.8; p =
              | .09
              | χ2(2) = 6.5; p =
              | .04
              | χ2(2) = 6.1; p =
              | .05
              | aStatistically signiﬁcant difference from close-ended (p < .05)bStatistically signiﬁcant
              | difference from open-ended with probing (p < .05)c Statistically signiﬁcant difference
              | from open-ended without probing (p < .05)
              | Table 4 also contains two other reasons that, while not socially undesirable, may be sensitive
              | to the method of questioning. These reasons include not advertising because it is too
              | complicated or because the respondent had never thought of advertising. Although these
              | reasons are not socially taboo, respondents may hesitate to report these reasons because
              | the reasons suggest that respondents have low levels of sophistication in running a
              | business. Never thinking of advertising was mentioned by 8% percent of respondents in the
              | close-ended group, more than the open-ended groups with probing (3%) and without probing
              | (5%). Similarly, 5% of the close-ended group said advertising was too complicated, higher
              | than both open-ended groups.
              | These three results support Hypothesis 3, that close-ended questions will yield more socially
              | undesirable responses. However, there is no support for Hypothesis 4, that probing allows
              | interviewers to build a rapport with respondents and is more likely to encourage socially
              | undesirable reporting.
              | Reasons for Not Switching Supplier
              | Table 5 shows the reasons respondents provided for not switching the business from whom
              | the respondent buys supplies or raw materials. The vast majority of respondents in all groups
              | reported not switching suppliers because they were satisﬁed with their current supplier.
              | There were no statistically signiﬁcant differences in the reasons provided by the three
              | experimental groups. It is possible that the highly skewed distribution of these reasons may
              | account for the absence of an effect.
              | Table 5. Reasons for Not Switching Supplier, by Experimental Group (Percentages)
              | Close-ended Open-ended
              | with probing
              | (n = 58)
              | (n = 67)
              | Open-ended
              | without
              | probing
              | (n = 62)
              | Overall χ2
              | 10 sur 14
              | 15.01.14 09:36
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
              | Satisﬁed with current
              | supplier
              | Quality is too poor
              | Finding a new supplier
              | takes too long
              | Too expensive
              | Too complicated to switch
              | Not available
              | Other
meta          | 86
              | 10
              | 9
              | 7
              | 7
              | 5
              | 2
              | 85
              | 4
              | 4
              | 1
              | 1
              | 6
              | 1
              | 84
              | 3
              | 3
              | 5
              | 2
              | 8
              | 2
text          | χ2(2) = 0.0; p =
              | .94
              | χ2(2) = 3.1; p =
              | .21
              | χ2(2) = 1.9; p =
              | .39
              | χ2(2) = 2.3; p =
              | .32
              | χ2(2) = 3.7; p =
              | .16
              | χ2(2) = 0.0; p =
              | .80
              | χ2(2) = 0.0; p =
              | .99
              | Reasons for Not Sharing Storage
              | In Table 6, we show the reasons that respondents provided for not sharing product storage
              | with another business. The sample sizes in this table are small because only respondents
              | who reported using storage (22% of the entire sample) were asked subsequent questions
              | about sharing storage.
              | Not trusting other businesses is a socially taboo topic because community cohesion is valued
              | in Ethiopia and openly discussing distrust of others is discouraged. This reason was
              | endorsed by 30% of the close-ended group, signiﬁcantly higher than the open-ended with
              | probing group (10%). Only 14% of the open-ended without probing group cited this reason.
              | This result supports Hypothesis 3 (close-ended responses will increase socially undesirable
              | reporting), but there is no support for Hypothesis 4 (that probing increases socially
              | undesirable reports). The experimental manipulation did not affect reports about “never
              | thought about it,” which contrasts with the results above for the reasons for not advertising.
              | Table 6. Reasons for Not Sharing Storage, by Experimental Group (Percentages)
              | Close-ended Open-ended Open-ended
              | with probing without
              | (n = 37) probing
              | Overall χ2
              | Never thought about it
              | Canʼt trust other
              | businesses
meta          | 38
text          | 30b
              | (n = 41)
meta          | 34
text          | 10 a
              | (n = 35)
meta          | 51
              | 14
text          | χ2(2) = 2.5; p =
              | .28
              | χ2(2) = 5.7; p =
              | .06
              | 11 sur 14
              | 15.01.14 09:36
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
              | Donʼt need to share
              | Canʼt ﬁnd other
              | businesses to share with
              | Cost savings are not
              | worth the effort
              | Laws prohibit sharing
              | Other
meta          | 22
              | 16
              | 8
              | 3
              | 3
              | 27
              | 5
              | 15
              | 0
text          | 15c
meta          | 29
              | 9
              | 6
              | 9
text          | 0 b
              | χ2(2) = 0.5; p =
              | .78
              | χ2(2) = 2.9; p =
              | .23
              | χ2(2) = 1.9; p =
              | .39
              | χ2(2) = 4.2; p =
              | .12
              | χ2(2) = 8.1; p =
              | .02
              | a Statistically signiﬁcant difference from close-ended (p < .05)b Statistically signiﬁcant
              | difference from open-ended with probing (p < .05)c Statistically signiﬁcant difference from
              | open-ended without probing (p < .05)
              | Discussion
              | Our goal was to investigate best practices for asking respondents about reasons for their
              | behavior. Respondents endorsed more responses when asked close-ended (versus
              | open-ended) questions. This ﬁnding suggests that close-ended questions may spark greater
              | engagement with the subject matter because respondents are forced to consider each option
              | on its own, rather than reporting “top of mind” responses. Close-ended responses also
              | produced higher rates of socially undesirable responses, suggesting that close-ended
              | responses may help to elicit attitudes on sensitive topics. Providing socially undesirable
              | reasons through close-ended questions may reduce the stigma of the response. An
              | alternative hypothesis is that respondents had simply never thought of that reason before.
              | We leave it to future research to distinguish between these explanations.
              | Second, probing did not affect the number of overall responses or the number of socially
              | undesirable responses provided. This lack of an effect is notable, particularly because the
              | professional interviewers had experience and training in probing. In fact, for two out of three
              | questions, probing leads to more “other” responses that could not be classiﬁed into existing
              | or new categories. It is possible that spending time on probing may not be an efﬁcient use of
              | interviewersʼ efforts, particularly because interviewer probing may also introduce additional
              | variance into estimates. However, additional studies based on larger sample sizes are
              | needed to replicate this null ﬁnding, particularly for more difﬁcult questions where probing
              | might be more effective. Future research could also investigate what types of probes are
              | most productive at eliciting sensitive data from respondents.
              | In sum, our results provide tentative support for the idea that close-ended questions without
              | probing are the preferred method of asking respondents to provide reasons for their
              | behavior, at least for this population and topic. We are limited, however, in that we do not
              | have a gold standard that could specify which of the three designs produces the most valid
              | data. Future research should investigate the validity of different methods, particularly the
              | assumption that the additional responses provided by the close-ended questions are
              | meaningful. Researchers should also consider the possibility is that there is no method that
              | provides “true” reports, but simply that the three methods collect different types of data. For
              | example, open-ended questions may produce reasons that are immediately accessible in
              | respondentʼs minds, whereas close-ended questions can obtain reactions to issues
              | respondents rarely consider in their day-to-day lives. Cognitive interviewing could be useful
              | 12 sur 14
              | 15.01.14 09:36
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
              | in understanding how respondents approach the response task of questions that ask about
              | reasons for behavior.
              | Our results demonstrate the feasibility of asking respondents questions about reasons for
              | their behavior, but also raise questions about the validity of these data. Rates of item
              | non-response were less than 1% for these items, suggesting that individuals are willing to
              | provide reasons for their behavior. However, our results cannot show whether the reasons
              | provided are accurate. Respondents may intentionally misreport reasons for their behavior or
              | otherwise rationalize their behavior. Citing reasons for behavior may be cognitively
              | burdensome, particularly when respondents do not often consciously think about why they do
              | (or do not) engage in behaviors. For example, asking about topics a respondent has not
              | considered before may encourage the respondent to create an answer on the spot, leading
              | to inconsistent responses across items designed to measure similar concepts (Wilson 2013).
              | Alternatively, interviewers may make errors when classifying responses. Because of these
              | limitations, we view respondent self-reports about why they engage in behaviors as one of
              | several possible research methods (in addition to qualitative research and experiments) that
              | policymakers could use when designing policies or programs to modify behavior.
              | We encourage future research on the best practices and validity of asking respondents
              | questions about reasons for their behavior, particularly in larger samples in different
              | populations and substantive content areas. Research could fruitfully investigate various
              | designs for designing these questions, such as using showcards for close-ended lists or
              | coding verbatim open-ended responses. Another promising avenue for future research is to
              | study how to ask questions about why individuals do engage in behaviors rather than why
              | they do not. We believe that future research about if and when it is appropriate to ask these
              | questions will ultimately beneﬁt policymakers who rely on social scientists to explain why
              | individuals engage in particular behaviors.
              | Appendix_Question Wording
              | [1] An alternative method is collecting verbatim responses to open-ended questions and then
              | coding the verbatim responses post-hoc, ideally with multiple coders. Although this method
              | may reduce coding errors, it is also time and labor intensive. The survey we analyze in this
              | paper was designed to be a rapid and low-cost survey for use in settings such as Ethiopia,
              | so it used immediate classiﬁcation methods by interviewers.
              | References
ref           | 1. Centers for Disease Control and Prevention (2012). National Health Interview Survey 2012
              | Questionnaires, English. Retrieved electronically on 20 December, 2013 from
              | http://www.cdc.gov/nchs/nhis/quest_data_related_1997_forward.htm.
              | 2. Converse, J. M., and H. Schuman. 1974. Conversations at Random: Survey Research as
              | Interviewers See It. Hoboken, NJ: John Wiley & Sons.
              | 3. Currivan, D., A. Nyman, C. F. Turner, and L. Biener. 2004. Does Telephone Audio
              | Computer-Assisted Self-Interviewing Improve the Accuracy of Prevalence Estimates of Youth
              | Smoking? Evidence from the UMass Tobacco Study. Public Opinion Quarterly 68:542-564.
              | 4. Fowler, F. J., and T. Mangione. 1990. Standardized Survey Interviewing:
              | Minimizing Interviewer-Related Error. Newbury Park: Sage.
              | 5. Fowler, F. J. 1995. Improving Survey Questions: Design and Evaluation. Thousand Oaks,
              | CA: Sage Publications.
text          | 13 sur 14
              | 15.01.14 09:36
              | Asking Survey Respondents about Reasons for Their Behavio...
              | http://surveyinsights.org/?p=2914&preview=true&preview_i...
ref           | 6. Geer, J. G. 1988. What Do Open-Ended Questions Measure? Public Opinion Quarterly 52:
              | 365-367.
              | 7. Geer, J. G. 1991. Do Open-Ended Questions Measure “Salient” Issues? Public Opinion
              | Quarterly 55: 360-370.
              | 8. Groves, R.M., F.J. Fowler, M. P. Couper, J. M. Lepkowski, E. Singer, and R. Tourangeau.
              | 2009. Survey Methodology. Hoboken, NJ: John Wiley & Sons.
              | 9. Heckathorn, D. D. 1997. Respondent-Driven Sampling: A New Approach to the Study of
              | Hidden Populations. Social Problems, 44:174-199.
              | 10. Pasek, J., and J. A. Krosnick. 2010. Optimizing Survey Questionnaire Design in Political
              | Science: Insights From Psychology. In Oxford Handbook of American Elections and Political
              | Behavior, ed. J. Leighley, 27-50. Oxford: Oxford University Press.
              | 11. Schober, M. F. 1998. Making Sense of Questions: An Interactional Approach.
              | In Cognition and Survey Research, eds. M. G. Sirken, D. J. Herrmann, S. Schechter, N.
              | Schwarz, J. M. Tanur, and R. Tourangeau. New York: Wiley.
              | 12. Schober, M. F., and F. G. Conrad. 1997. Does Conversational Interviewing Reduce
              | Survey Measurement Error? Public Opinion Quarterly 61:576-602.
              | 13. Schober, M. F., and F. G. Conrad. 2002. A Collaborative View of Standardized Survey
              | Interviews. In Standardization and Tacit Knowledge: Interaction and Practice in the Survey
              | Interview, eds. D. Maynard, H. Houtkoop-Steenstra, N. C. Schaeffer, and J. van der Zouwen.
              | New York: Wiley.
              | 14. Schaeffer, N. C., and D. W. Maynard. 2008. The Contemporary Standardized Survey
              | Interview for Social Research. In Envisioning the Survey Interview of the Future, ed. F. G.
              | Conrad and M. Schober. New York: Wiley.
              | 15. Smyth, J.D., D.A. Dillman, L. M. Christian, and M.J. Stern. 2006. Comparing Check-all
              | and Forced-Choice Question Formats in Web Surveys. Public Opinion Quarterly, 70:66-77.
              | 16. Tourangeau, R. and Smith, T. W. 1996. Asking Sensitive Questions: The Impact of Data
              | Collection Mode, Question Format, and Question Context. Public Opinion Quarterly, 60:
              | 275-305.
              | 17. United States Census Bureau (2010). Current Population Survey Voting and Registration
              | Supplement Questionnaire, November, 2010. Retrieved electronically on 20 December, 2013
              | from http://www.census.gov/cps/methodology/techdocs.html.
              | 18. Wilson, S. 2010. Cognitive Interview Evaluation of the 2010 National Health Interview
              | Survey Supplement on Cancer Screenings & Survivorship: Results of interviews conducted
              | October – December, 2008. National Center for Health Statistics. Hyattsville, MD.
              | 19. Willson, S. 2013. Cognitive Interview Evaluation of the Federal Statistical System Trust
              | Monitoring Survey, Round 1: Results of interviews conducted in October, 2011. National
              | Center for Health Statistics. Hyattsville, MD.
text          | 14 sur 14
              | 15.01.14 09:36