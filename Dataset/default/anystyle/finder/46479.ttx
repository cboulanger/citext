text          | Improving Retrieval Results with Discipline-Specific
              | Query Expansion
              | Thomas Lüke, Philipp Schaer, and Philipp Mayr
              | GESIS – Leibniz Institute for the Social Sciences,
              | Unter Sachsenhausen 6-8, 50667 Cologne, Germany
              | {thomas.lueke,philipp.schaer,philipp.mayr}@gesis.org
              | Abstract. Choosing the right terms to describe an information need is becom-
              | ing more difficult as the amount of available information increases. Search-
              | Term-Recommendation (STR) systems can help to overcome these problems.
              | This paper evaluates the benefits that may be gained from the use of STRs in
              | Query Expansion (QE). We create 17 STRs, 16 based on specific disciplines
              | and one giving general recommendations, and compare the retrieval perfor-
              | mance of these STRs. The main findings are: (1) QE with specific STRs leads
              | to significantly better results than QE with a general STR, (2) QE with specific
              | STRs selected by a heuristic mechanism of topic classification leads to better
              | results than the general STR, however (3) selecting the best matching specific
              | STR in an automatic way is a major challenge of this process.
              | Keywords: Term Suggestion, Information Retrieval, Thesaurus, Query Expan-
              | sion, Digital Libraries, Search Term Recommendation.
meta          | 1
text          | Introduction
              | Users of scientific digital libraries have to deal with a constantly growing amount of
              | accessible information. The challenge of expressing one’s information need through
              | the right terms has been described as “vocabulary problem” by Furnas et al. [1]. Spe-
              | cialized knowledge organization systems (KOS) like thesauri or classifications have
              | been created to support the users in the search process and to provide a consistent way
              | of expressing the information need. As a way to provide easy access to these tools
              | methods like Entry Vocabulary Modules [6] or so-called Search-Term-
              | Recommenders (STR) have been introduced. Such recommendation services are also
              | in use in commercial end-user systems like Amazon or eBay.
              | A common approach in IR systems to improve retrieval results is the use of term
              | based query expansion (QE). If semantically close concepts are added to an initial
              | query term it often shows query results improvements. Typically language and termi-
              | nology becomes more specific if a terminology in a subject discipline e.g. the social
              | sciences is differentiated into sub disciplines. This has also been proven for sugges-
              | tions given by STRs [7], however, effects on the application of QE have not been
              | evaluated. Using a heuristic mechanism, our approach utilizes the phenomenon of
              | P. Zaphiris et al. (Eds.): TPDL 2012, LNCS 7489, pp. 408–413, 2012.
              | © Springer-Verlag Berlin Heidelberg 2012
              | Improving Retrieval Results with Discipline-Specific Query Expansion
meta          | 409
text          | language specialization to recommend the most specific concepts from a controlled
              | vocabulary through discipline-specific STRs. We conduct an empirical analysis in the
              | domain of social sciences to evaluate retrieval performance in a standard IR evalua-
              | tion environment using QE.
              | The paper is structured as follows: Section 2 gives a brief overview of previous
              | findings in the area of search term recommendation, query expansion and IR evalua-
              | tion. Section 3 describes the methods used in this paper to evaluate our setup. In Sec-
              | tion 4 we present and discuss our evaluation. Section 5 summarizes the findings of
              | this paper and presents ideas for future work.
meta          | 2
text          | Related Work
              | Hargittai [2] has shown that users need supporting mechanisms while expressing their
              | information need through search queries. Such support may be provided by query
              | recommendation mechanisms, which try to enrich the existing query with additional
              | terms. This leads to better retrieval results or provides the searcher with a new view-
              | point on the search as shown by Mutschke et al. [5].
              | Automatic Query Expansion mechanisms have been divided into two classes by
              | Xu and Croft [8]: Expansion recommendations based on a global analysis of the entire
              | document collection and recommendations based on the local subset of documents
              | that were retrieved by using the unexpanded query (called pseudo-relevance feedback
              | [4]). In their research the local approach outperformed the global one clearly. Howev-
              | er, when the amount of non-relevant documents in the results to the original query
              | increases, a so-called query drift may occur. Documents not relevant to the users’
              | information need lead to mostly irrelevant expansion terms. If the query gets ex-
              | panded with such terms, it drifts away from the original meaning and results in even
              | less relevant documents. Mitra et. al [4] have proposed techniques to overcome query
              | drift.
              | Additionally Petras [7] found that discipline-specific search term recommenders
              | which are trained on sub disciplinary document corpora deliver more specific search
              | term suggestions than general recommenders which are trained on an entire database.
meta          | 3
text          | Discipline-Specific Term Recommendation
              | In our approach we apply Petras’ [7] idea of more specific STRs onto an an automatic
              | query expansion setup. According to the findings in [4, 8] we expect retrieval results
              | to improve. With real-world applications in mind and the intention to reduce the
              | chance of query drift through non-relevant data sets we demand STRs to be created a-
              | priori rather than on-demand as with pseudo-relevance feedback. To create document
              | sets belonging to specific disciplines we use a hierarchically structured classification
              | system. It is called “classification of social sciences” and part of the SOLIS1 data set.
              | 1 http://www.gesis.org/en/services/research/solis-social-science-
              | literature-information-system/
meta          | 410
text          | T. Lüke, P. Schaer, and P. Mayr
              | For example any class starting with 1 is connected to the entire field of social
              | sciences, any class starting with 102 is connected to the sub-discipline of sociology
              | and class 10209 is assigned to documents from the special field of family sociology.
              | This structure allowed us to create STRs at the top level that cover every area of the
              | classification system thereby allowing us to choose matching discipline-specific STRs
              | for queries from various disciplines.
              | Table 1. Overview of the created discipline-specific (DS) STRs. Class describes the specific
              | sub-discipline of the social sciences the STR was based on. #Docs and #CT shows the number
              | of documents or controlled vocabulary terms in that given collection.
              | STR
              | DS-1
              | DS-2
              | DS-3
              | DS-4
              | DS-5
              | DS-6
              | DS-7
              | DS-8
              | Global
              | Class
              | Basic Research
              | Sociology
              | Demography
              | Ethnology
              | Political
              | Science
              | Education
              | Psychology
              | Communications
              | Social sciences
              | #Docs
meta          | 26817
              | 76342
              | 26298
              | 5409
              | 95536
              | 18820
              | 24785
              | 37285
              | 383000
text          | #CT
meta          | 5642
              | 7184
              | 5322
              | 3787
              | 6995
              | 5199
              | 5725
              | 5893
              | 7781
text          | STR
              | DS-9
              | DS-10
              | DS-11
              | DS-12
              | DS-13
              | DS-14
              | DS-15
              | DS-16
              | Class #Docs
              | Economics 45217
              | Social Policy 26289
              | Employment 61742
              | Research
              | Women's Studies 18116
              | Interdisciplinary Fields 38985
              | Humanities
              | Legal Science
              | Natural Science
meta          | 53863
              | 16330
              | 6083
text          | #CT
meta          | 6213
              | 5586
              | 5610
              | 5301
              | 6454
              | 6703
              | 5549
              | 4015
text          | To test the effects of recommendation terms from different disciplines we use a
              | standard IR evaluation environment. A set of pre-defined topics, each consisting of a
              | title and a set of documents relevant to that query, is processed on the SOLR2 search
              | platform which uses tf-idf to rank results. The data sets that represent the basis for the
              | different STRs are all created from the SOLIS dataset, a collection of more than
              | 400,000 documents from various disciplines of the social sciences.
              | Based on the assumption of a specialized vocabulary in different scientific discip-
              | lines, 16 custom data sets (see DS-1 to DS-16 in Table 1) for different sub-disciplines
              | of SOLIS are created. These 16 sets as well as the entire SOLIS data set (see Global
              | in Table 1) are the basis for 17 STRs.
              | In addition SOLIS is indexed with the thesaurus TheSoz3 which consists of almost
              | 7800 descriptor terms. Our discipline-specific STRs reduce the vocabulary of TheSoz
              | to about 5700 terms per data set (on average) with a trend of even smaller (and pre-
              | sumably more specific) vocabularies. Exact numbers can be found in Table 1.
              | Each STR is created to match arbitrary input terms to terms of the controlled voca-
              | bulary TheSoz. All documents of a data set are processed using a co-occurrence anal-
              | ysis of input terms (found in title and abstract of each document) and the subject-
              | specific descriptor terms assigned to a document. In order to rank the suggested terms
              | from the controlled vocabulary the logarithmically modified Jaccard similarity meas-
              | ure is used.
              | 2 http://lucene.apache.org/solr/
              | 3 http://thedatahub.org/dataset/gesis-thesoz
              | Improving Retrieval Results with Discipline-Specific Query Expansion
meta          | 411
              | 4
text          | Evaluation
              | This section contains a description of our evaluation. In the first paragraph the setup is
              | described. The second paragraph presents the main results and effects of discipline-
              | specific expansion on retrieval performance measured through average precision val-
              | ues. In addition we give a single example of a query, going from the broader level to a
              | detailed in-depth inspection of discipline-specific QE.
              | 4.1
              | Evaluation Setup
              | In order to test the effects of discipline-specific STRs and a general STR within a
              | query expansion scenario we choose the GIRT4 corpus [3], which is a subset of
              | SOLIS. It is used in evaluation campaigns like CLEF or TREC. We use 100 of the
              | CLEF topics ranging from years 2003 to 2006 (topic numbers were 76 to 175) and
              | classify them through a heuristic approach based on the classification system men-
              | tioned above: All relevant documents for a given topic are put into groups based on
              | their classification ID. The classification ID of the group that holds the most docu-
              | ments is assumed to be the topic’s classification. Queries from these topics are created
              | by removing stop words from the title of each topic. To test the performance of QE
              | we expand the query with three different STRs:
              | •
              | •
              | •
              | the general STR, based on the entire SOLIS data set (our baseline)
              | the STR of the topic’s class
              | the STR that performs best for a given topic (out of the 16 discipline-specific
              | STRs)
              | Every QE is performed automatically with the top 4 recommendations of a STR. We
              | report mean average precision (MAP), rPrecision as well as p@5, p@10, p@20 and
              | p@30. As additional comparison we include the results of a standard installation of
              | SOLR without QE. Every query, whether expanded or not, is processed by this plat-
              | form. We use Student’s t-test to verify significance of the improvements.
              | 4.2
              | Results
              | Table 2 shows the results of the QE with different STRs. The first observation which
              | can be made is that the discipline-specific STRs always perform better than a general
              | STR (and thereby also improve retrieval performance compared to an unexpanded
              | query). The last line of Table 2 shows the maximum performance possible through
              | the use of our 16 discipline-specific STRs. It is significantly better than a general STR
              | in every case. However, the improvements for those STRs that are chosen based on
              | the classification of topics did not always reach significance. Only the precision with-
              | in the first 5 and 10 top ranked documents is significantly higher. Still a more precise
              | classification of the original query is necessary to gain maximum benefit from our
              | discipline-specific STRs. In addition to measuring the impact on average precision we
              | further analyze the use of discipline-specific STRs by examining an individual query.
              | According to Petras [6] the results of a QE could significantly improve if the “right”
meta          | 412
text          | T. Lüke, P. Schaer, and P. Mayr
              | Table 2. Evaluation results averaged over 100 topics for three different types of QE.
              | Recommendations are based on a general STR (gSTR) which served as a baseline, a discipline-
              | specific STR fitting the class of the topic (tSTR) and the discipline-specific STR performing
              | best on each topic (bSTR). Confidence levels of significance are: * α = .05, ** α = .01.
              | Exp. Type MAP
              | gSTR (Base) 0.155
              | tSTR 0.159
              | bSTR 0.179**
              | rPrecison
              | 0.221
              | 0.224
              | 0.233**
              | p@5
              | 0.548
              | 0.578*
              | 0.658**
              | p@10
              | 0.509
              | 0.542**
              | 0.601**
              | p@20
              | 0.449
              | 0.460
              | 0.512**
              | p@30
              | 0.420
              | 0.424
              | 0.463**
              | Table 3. Top 4 recommendations for the input terms “bilingual education” from three STRs
              | Recommendation
meta          | 1
              | 2
              | 3
              | 4
text          | General
              | Multilingualism
              | Child
              | Speech
              | Intercultural Education
              | Topic-fitting
              | Child
              | School
              | Multilingualism
              | Germany
              | Best-performing
              | Multilingualism
              | Speech
              | Ethnic Group
              | Minority
              | terms are added to the query. We will see how different recommendations influence
              | the results. Topic no. 131 has the title (and thus the query) “bilingual education”.
              | Table 3 shows the top 4 recommendations of each STR for this topic. While “multi-
              | lingualism” is always a recommendation and “child” and “speech” appear twice, the
              | rest of the terms appear only in one recommender. The general recommender propos-
              | es the most common terms while the two discipline-specific STRs propose more spe-
              | cific terms. In Table 4 we can see the effects of these different recommendations on
              | retrieval precision. The unexpanded query performs satisfying but leaves room for
              | improvement as it presents only 2 relevant documents within the top 5 and 3 within
              | the top 10 documents (see p@5 and p@10). Expanding the query with terms from the
              | general STR improves these results to 3 and 6 relevant documents in the top 5 or top
              | 10 respectively. Using terms from the pre-defined, topic-fitting, discipline-specific
              | STR 4 out of 5 documents within the top 5 are relevant. Finally, the best performing
              | discipline-specific STR manages to expand the query in a way that all top 10 docu-
              | ments are relevant and even within the top 20 documents only 3 are not relevant.
              | Table 4. Evaluation results for topic 131 with three different types of QE Recommendations.
              | Bold font indicates improvement
              | Exp. Type
              | Solr
              | gSTR
              | tSTR
              | bSTR
              | AP
              | 0.039
              | 0.072
              | 0.076
              | 0.147
              | rPrecison
              | 0.127
              | 0.144
              | 0.161
              | 0.161
              | p@5
              | 0.4
              | 0.6
              | 0.8
meta          | 1
              | 5
text          | Conclusion and Future Work
              | p@10
              | 0.3
              | 0.6
              | 0.6
meta          | 1
text          | p@20
              | 0.2
              | 0.4
              | 0.45
              | 0.85
              | p@30
              | 0.133
              | 0.333
              | 0.333
              | 0.567
              | Our research shows that the use of discipline-specific Search-Term-Recommenders
              | can improve the retrieval performance significantly if used as basis for an automated
              | Improving Retrieval Results with Discipline-Specific Query Expansion
meta          | 413
text          | query expansion. However, it also becomes clear that choosing the best STR in an
              | automated setting of query expansion is far from trivial. By doing an in-depth analysis
              | of a single query we additionally demonstrate how discipline-specific term recom-
              | mendations can improve the quality of search results for a user. This leads us to the
              | conclusion that discipline-specific STRs can be a valuable addition to expert search
              | platforms where users might not know how to optimally express their search.
              | In conclusion, STRs that are meant to assist users should be discipline-specific in
              | order to recommend more specific terms. Still, it has to be determined how specific
              | (or small) a data set may be while still producing reasonable results. To improve qual-
              | ity of QE it is essential to have a good algorithm for determining the specific discip-
              | line of the query. Besides having more specific recommendation another aspect of
              | further research could be the use of additional metadata fields as it is common for
              | users to enrich their search by explicitly specifying authors or other metadata fields
              | (for further research on recommendations based on different types of metadata see the
              | work by Schaer et al. in these proceedings). A STR providing recommendations of
              | this kind could add additional benefits to a user’s search, especially if it recommends
              | e.g. the main authors of a specific discipline.
              | Acknowledgements. This work was partly funded by DFG, grant no. SU 647/5-2.
              | References
ref           | 1. Furnas, G.W., et al.: The Vocabulary Problem in Human-System Communication. Commu-
              | nications of the ACM 30(11), 964–971 (1987)
              | 2. Hargittai, E.: Hurdles to information seeking: Spelling and typographical mistakes during
              | users’ online behavior. Journal of the Association of Information Systems 7(1), 52–67
              | (2006)
              | 3. Kluck, M.: The GIRT Data in the Evaluation of CLIR Systems – from 1997 Until 2003. In:
              | Peters, C., Gonzalo, J., Braschler, M., Kluck, M., et al. (eds.) CLEF 2003. LNCS, vol. 3237,
              | pp. 376–390. Springer, Heidelberg (2004)
              | 4. Mitra, M., et al.: Improving automatic query expansion. In: Proceedings of the 21st Annual
              | International ACM SIGIR Conference on Research and Development in Information Re-
              | trieval, pp. 206–214. ACM, Melbourne (1998)
              | 5. Mutschke, P., et al.: Science models as value-added services for scholarly information sys-
              | tems. Scientometrics 89(1), 349–364 (2011)
              | 6. Petras, V.: How one word can make all the difference - using subject metadata for auto-
              | matic query expansion and reformulation. In: Working Notes for the CLEF 2005 Workshop,
              | Vienna, Austria, September 21-23 (2005)
              | 7. Petras, V.: Translating Dialects in Search: Mapping between Specialized Languages of Dis-
              | course and Documentary Languages. University of California (2006)
              | 8. Xu, J., Croft, W.B.: Query expansion using local and global document analysis. In: Pro-
              | ceedings of the 19th Annual International ACM SIGIR Conference on Research and Devel-
              | opment in Information Retrieval, pp. 4–11. ACM, Zurich (1996)